{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Write a program to demonstrate the working of EM algorithm. Apply EM algorithm to\n",
        "cluster a set of data stored in a .CSV file."
      ],
      "metadata": {
        "id": "qwiYxMl8HQMr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQq6RsDeHIFE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from scipy.stats import norm\n",
        "import seaborn as sns\n",
        "\n",
        "mu1, sigma1 = 2, 1\n",
        "mu2, sigma2 = -1, 0.8\n",
        "X1 = np.random.normal(mu1, sigma1, size=200)\n",
        "X2 = np.random.normal(mu2, sigma2, size=600)\n",
        "X = np.concatenate([X1, X2])\n",
        "X = X.reshape(-1, 1)\n",
        "gmm = GaussianMixture(n_components=2, random_state=0)\n",
        "gmm.fit(X)\n",
        "\n",
        "x_grid = np.linspace(min(X), max(X), 1000).reshape(-1, 1)\n",
        "density_estimation = np.exp(gmm.score_samples(x_grid))\n",
        "sns.kdeplot(X, label = \"Actual Density\")\n",
        "plt.plot(x_grid, density_estimation, label='Estimated density\\nFrom Gaussian Mixture Model')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Density')\n",
        "plt.title('Density Estimation using GMM')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a program to demonstrate the working of Na√Øve bayes classifier by considering\n",
        "an appropriate textual data set and calculate the accuracy for the same."
      ],
      "metadata": {
        "id": "Lf7Lqu10HUu4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "naive_bayes_classifier = GaussianNB()\n",
        "\n",
        "naive_bayes_classifier.fit(X_train, y_train)\n",
        "\n",
        "y_pred = naive_bayes_classifier.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "classification_report_result = classification_report(y_test, y_pred, target_names=iris.target_names)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"\\nClassification Report:\\n\", classification_report_result)"
      ],
      "metadata": {
        "id": "yl0b10OkHago"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use appropriate dataset for clustering using k-Means algorithm. You can add Python ML\n",
        "library classes/API in the program."
      ],
      "metadata": {
        "id": "wLTW7FQuHcLf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "data, true_labels = make_blobs(n_samples=300, centers=3, random_state=42)\n",
        "\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "predicted_labels = kmeans.fit_predict(data)\n",
        "centroids = kmeans.cluster_centers_\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(data[:, 0], data[:, 1], c=true_labels, cmap='viridis', edgecolors='k', s=50)\n",
        "plt.title('True Clusters')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(data[:, 0], data[:, 1], c=predicted_labels, cmap='viridis', edgecolors='k', s=50)\n",
        "plt.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='X', s=200, label='Centroids')\n",
        "plt.title('K-Means Clusters')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7zUVQnWDHhKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a program to demonstrate the working of Association rule learning using Apriori\n",
        "Algorithm by using appropriate data set."
      ],
      "metadata": {
        "id": "ho2x3T44HiQR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import mlxtend\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "from mlxtend.frequent_patterns import apriori, association_rules\n",
        "import pandas as pd\n",
        "\n",
        "transactions = [['Milk', 'Bread', 'Butter'],\n",
        "                ['Milk', 'Bread'],\n",
        "                ['Milk', 'Diapers'],\n",
        "                ['Milk', 'Beer', 'Diapers'],\n",
        "                ['Bread', 'Butter'],\n",
        "                ['Bread', 'Beer'],\n",
        "                ['Butter', 'Diapers'],\n",
        "                ['Bread', 'Beer', 'Diapers']]\n",
        "\n",
        "encoder = TransactionEncoder()\n",
        "onehot = encoder.fit_transform(transactions)\n",
        "df = pd.DataFrame(onehot, columns=encoder.columns_)\n",
        "\n",
        "frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
        "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.7)\n",
        "\n",
        "print(\"Frequent Itemsets:\")\n",
        "print(frequent_itemsets)\n",
        "\n",
        "print(\"Association Rules:\")\n",
        "print(rules)"
      ],
      "metadata": {
        "id": "iNTO3ys8HkS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a program to demonstrate the working of Principle Component Analysis."
      ],
      "metadata": {
        "id": "9BoAkWjmHnAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data = {\n",
        "    'Feature1': [1, 2, 3, 4, 5],\n",
        "    'Feature2': [5, 4, 3, 2, 1],\n",
        "    'Feature3': [2, 3, 4, 5, 6]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "df_scaled = scaler.fit_transform(df)\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "principal_components = pca.fit_transform(df_scaled)\n",
        "\n",
        "df_pca = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])\n",
        "\n",
        "print(\"Explained Variance Ratio:\")\n",
        "print(pca.explained_variance_ratio_)\n",
        "\n",
        "formatted_principal_components = np.array2string(principal_components, precision=2, separator=', ')\n",
        "print(\"Principal Components:\", formatted_principal_components)\n",
        "\n",
        "plt.scatter(df_pca['PC1'], df_pca['PC2'])\n",
        "plt.title('PCA: Principal Components')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YpJqHYZGHtgY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}